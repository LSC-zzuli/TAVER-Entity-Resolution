{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5341a56f",
   "metadata": {},
   "source": [
    "note: 这个代码是为了测试sentence embedding model的效果--将每个属性分开训练向量化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e7417d",
   "metadata": {},
   "source": [
    "## 1 训练sentence embedding model，将元组转换为向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683b820",
   "metadata": {},
   "source": [
    "### 1.1 加载数据，取出无关元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80459219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")  #忽略告警\n",
    "import os\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5  # 程序最多只能占用指定gpu50%的显存\n",
    "# config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "# sess = tf.Session(config = config)\n",
    "\n",
    "file_path = 'data/Structured/DBLP-Scholar/'\n",
    "\n",
    "table_a_df = pd.read_csv(file_path + 'tableA.csv')\n",
    "table_b_df = pd.read_csv(file_path + 'tableB.csv')\n",
    "train_df = pd.read_csv(file_path + 'train.csv')\n",
    "valid_df = pd.read_csv(file_path + 'valid.csv')\n",
    "test_df = pd.read_csv(file_path + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86225f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17223, 3), (5742, 3), (5742, 3), (2616, 5), (64263, 5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, valid_df.shape, table_a_df.shape, table_b_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270e029c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>towards a cooperative transaction model - the ...</td>\n",
       "      <td>m rusinkiewicz , w klas , t tesch , j wфsch , ...</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sql/xml is making good progress</td>\n",
       "      <td>a eisenberg , j melton</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>using formal methods to reason about semantics...</td>\n",
       "      <td>p ammann , s jajodia , i ray</td>\n",
       "      <td>vldb</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>editor 's notes</td>\n",
       "      <td>l liu</td>\n",
       "      <td>sigmod record</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>report on the acm fourth international worksho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  towards a cooperative transaction model - the ...   \n",
       "1   1                    sql/xml is making good progress   \n",
       "2   2  using formal methods to reason about semantics...   \n",
       "3   3                                    editor 's notes   \n",
       "4   4  report on the acm fourth international worksho...   \n",
       "\n",
       "                                             authors          venue  year  \n",
       "0  m rusinkiewicz , w klas , t tesch , j wфsch , ...           vldb  1995  \n",
       "1                             a eisenberg , j melton  sigmod record  2002  \n",
       "2                       p ammann , s jajodia , i ray           vldb  1995  \n",
       "3                                              l liu  sigmod record  2002  \n",
       "4                                                NaN            NaN  2002  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f851c38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ltable_id</th>\n",
       "      <th>rtable_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2563</td>\n",
       "      <td>6425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1861</td>\n",
       "      <td>64163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2187</td>\n",
       "      <td>64008</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>611</td>\n",
       "      <td>15632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>631</td>\n",
       "      <td>27684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ltable_id  rtable_id  label\n",
       "0       2563       6425      0\n",
       "1       1861      64163      1\n",
       "2       2187      64008      1\n",
       "3        611      15632      0\n",
       "4        631      27684      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83491f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                       730\n",
       "title      reconciling schemas of disparate data sources ...\n",
       "authors                       a doan , p domingos , a halevy\n",
       "venue                                      sigmod conference\n",
       "year                                                    2001\n",
       "Name: 730, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_a_df.loc[730,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac116b",
   "metadata": {},
   "source": [
    "## 2 构建AVB模型，无监督训练encoder,用于后续的分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1c4629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.contrib import layers as tflayers\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25720a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全局超参数变量\n",
    "global max_batch_n, batch_index\n",
    "global LR_PRIMAL, LR_DUAL, X_dim, h_dim, z_dim, eps_dim, eps_nbasis\n",
    "global encoder, decoder, discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1f49a",
   "metadata": {},
   "source": [
    "### 2.1 加载相关所需参数和函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "900e1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个将所有句子，转换为tuple vector 的函数\n",
    "def TupleEmbedding(all_sentences, sentence_embedding_model):\n",
    "    '''\n",
    "    all_sentences: 包含所有元组句子的列表\n",
    "    sentence_embedding_model: 将句子转换为句子向量的模型\n",
    "    founction：将所有的句子，转换为对应的sentence vector\n",
    "    '''\n",
    "    sentences_vector = []\n",
    "    for sen in all_sentences:\n",
    "        sentences_vector.append(sentence_embedding_model.encode(sen))\n",
    "    return np.array(sentences_vector)\n",
    "\n",
    "# 构造一个产生分批数据的函数\n",
    "def GetBatchData(train_loader):\n",
    "    '''\n",
    "    传入torch的dataloader,然后生成对应的分批数据\n",
    "    '''\n",
    "    global batch_index\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        if step != batch_index:\n",
    "            continue\n",
    "        # batch_index = (batch_index + 1) % max_batch_n \n",
    "        # 对batch_x的维度进行修改，转换为（batch_size, -1）\n",
    "        sample_x = batch_x.view(BATCH_SIZE, -1).data.numpy()\n",
    "    return sample_x\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return tf.maximum(x, leak*x)\n",
    "\n",
    "# 模型计算图构建当中，所需的相关函数\n",
    "def get_zlogprob(z):\n",
    "    # temp = tf.clip_by_value(predict_1, 1e-8, 1.0)\n",
    "    logprob = -0.5 * tf.reduce_sum(z*z  + np.log(2*np.pi), [1])\n",
    "    return logprob\n",
    "\n",
    "def bit_product_sum(x, y):\n",
    "    return sum([item[0] * item[1] for item in zip(x, y)])\n",
    "\n",
    "def get_reconstr_err(de_x, x):\n",
    "    x_multi = tf.multiply(x, x)\n",
    "    de_x_multi = tf.multiply(de_x, de_x)\n",
    "    x_de_x_multi = tf.multiply(x, de_x)\n",
    "\n",
    "    x_mul_sum = tf.reduce_sum(x_multi)\n",
    "    de_x_mul_sum = tf.reduce_sum(de_x_multi)\n",
    "    x_de_mul_sum = tf.reduce_sum(x_de_x_multi)\n",
    "\n",
    "    cosine = tf.divide(x_de_mul_sum, tf.add(x_mul_sum, de_x_mul_sum))\n",
    "    reconst_err = tf.subtract(2., cosine)\n",
    "\n",
    "    return reconst_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c3f83",
   "metadata": {},
   "source": [
    "### 2.2 构建编码器、解码器、判别器函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202f643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建编码器，解码器，判别器模型\n",
    "def encoder_func(x):\n",
    "    size = x.shape[0]\n",
    "    # eps = tf.random_normal(tf.stack([eps_nbasis, size, eps_dim]))\n",
    "\n",
    "    net = slim.fully_connected(x, h_dim, activation_fn=tf.nn.softplus)\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.softplus)\n",
    "    # net = slim.dropout(net, 0.3, is_training=False)\n",
    "    # net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.relu)\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.softplus)\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.softplus)\n",
    "\n",
    "    z_mean = slim.fully_connected(net, z_dim, activation_fn=None)\n",
    "    z_logstd = slim.fully_connected(net, z_dim, activation_fn=None)\n",
    "    # logstd = log(std)\n",
    "    return z_mean, z_logstd\n",
    "\n",
    "def decoder_func(z):\n",
    "    net = z\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.softplus)\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.softplus)\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.softplus)\n",
    "    xlogits = slim.fully_connected(net, X_dim, activation_fn=None)\n",
    "    return xlogits\n",
    "\n",
    "def discriminator_func(x, z):\n",
    "    # Theta\n",
    "    with tf.variable_scope(\"theta\"):\n",
    "        fc_argscope = slim.arg_scope([slim.fully_connected], activation_fn=lrelu)\n",
    "        with fc_argscope:\n",
    "            net = slim.fully_connected(x, 1024, scope='fc_0')\n",
    "            net = slim.fully_connected(net, 1024, scope='fc_1')\n",
    "            net = slim.fully_connected(net, 1024, scope='fc_2')\n",
    "        theta = slim.fully_connected(net, 8192, activation_fn=tf.nn.elu, scope='theta',\n",
    "                  weights_initializer=tf.truncated_normal_initializer(stddev=1e-5))\n",
    "\n",
    "    with tf.variable_scope(\"s\"):\n",
    "        fc_argscope = slim.arg_scope([slim.fully_connected],\n",
    "          activation_fn=lrelu)\n",
    "        with fc_argscope:\n",
    "            net = slim.fully_connected(z, 1024, scope='fc_0')\n",
    "            net = slim.fully_connected(net, 1024, scope='fc_1')\n",
    "            net = slim.fully_connected(net, 1024, scope='fc_2')\n",
    "        s = slim.fully_connected(net, 8192, activation_fn=None, scope='s')\n",
    "\n",
    "    with tf.variable_scope(\"xonly\"):\n",
    "        fc_argscope = slim.arg_scope([slim.fully_connected],\n",
    "          activation_fn=lrelu)\n",
    "        with fc_argscope:\n",
    "            net = slim.fully_connected(x, 1024, scope='fc_0')\n",
    "            net = slim.fully_connected(net, 1024, scope='fc_1')\n",
    "            net = slim.fully_connected(net, 1024, scope='fc_2')\n",
    "        Tx = slim.fully_connected(net, 1, activation_fn=None, scope='Tx',\n",
    "              weights_initializer=tf.truncated_normal_initializer(stddev=1e-5))\n",
    "\n",
    "    with tf.variable_scope(\"zonly\"):\n",
    "        fc_argscope = slim.arg_scope([slim.fully_connected],\n",
    "          activation_fn=lrelu)\n",
    "        with fc_argscope:\n",
    "            net = slim.fully_connected(z, 1024, scope='fc_0')\n",
    "            net = slim.fully_connected(net, 1024, scope='fc_1')\n",
    "            net = slim.fully_connected(net, 1024, scope='fc_2')\n",
    "        Tz = slim.fully_connected(net, 1, activation_fn=None, scope='Tz',\n",
    "              weights_initializer=tf.truncated_normal_initializer(stddev=1e-5))\n",
    "\n",
    "    T = tf.reduce_sum(theta * s, [1], keep_dims=True) + Tx + Tz\n",
    "    T = tf.squeeze(T, 1)\n",
    "    T += 0.5 * tf.reduce_sum(tf.square(z), [1])\n",
    "    return T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3ee53",
   "metadata": {},
   "source": [
    "### 2.3 构建模型训练计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b86ece12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建计算图\n",
    "\n",
    "def BuildComputationalGraph(x_real):\n",
    "    '''\n",
    "    构建计算图，输出两个损失值，和对应的参数\n",
    "    '''\n",
    "    # x_real = GetBatchData(train_loader)\n",
    "    # x_real = tf.placeholder(tf.float32, shape=[BATCH_SIZE, X_dim])\n",
    "    z_sampled = tf.random_normal([BATCH_SIZE, z_dim])\n",
    "    eps = tf.random_normal([BATCH_SIZE, z_dim])\n",
    "\n",
    "    # z_real, Ez, Varz = encoder(x_real) # 编码器的输出\n",
    "    mean, logstd = encoder(x_real)\n",
    "    std = tf.exp(logstd)\n",
    "    z_real = mean + eps*std\n",
    "\n",
    "    z_mean = tf.stop_gradient(mean)\n",
    "    var = tf.multiply(std, std)\n",
    "    z_var = tf.stop_gradient(var)\n",
    "    z_std = tf.stop_gradient(std)\n",
    "\n",
    "    z_norm = (z_real - z_mean) / z_std\n",
    "    # z_norm = z_real\n",
    "    Td = discriminator(x_real, z_norm) # 后验\n",
    "    Ti = discriminator(x_real, z_sampled) # 先验\n",
    "    logz = -0.5 * tf.reduce_sum(z_real*z_real  + np.log(2*np.pi), [1])\n",
    "    # # 防止log操作出现nan\n",
    "    z_var = tf.clip_by_value(z_var, 1e-8, 1.0)\n",
    "    logr = -0.5 * tf.reduce_sum(z_norm*z_norm + tf.log(z_var) + np.log(2*np.pi), [1])\n",
    "\n",
    "    decoder_out = decoder(z_real)  # 解码器的输出\n",
    "\n",
    "    # 计算图所需参数\n",
    "    c_dim = 1\n",
    "    beta = 1\n",
    "    factor = 10.0 / (X_dim * c_dim)\n",
    "\n",
    "    # # Primal loss\n",
    "    reconst_err = get_reconstr_err(decoder_out, x_real)\n",
    "    KL = Td + logr - logz\n",
    "#     KL = Td\n",
    "    # KL = tf.reduce_sum(0.5*tf.square(z_real) - logstd - 0.5, 1)\n",
    "    ELBO = reconst_err + KL\n",
    "    #可以适当增加beta 和 factor的大小\n",
    "    loss_primal = factor * tf.reduce_mean(3*reconst_err + beta*KL)\n",
    "\n",
    "    # Mean values\n",
    "    ELBO_mean = tf.reduce_mean(ELBO)\n",
    "    KL_mean = tf.reduce_mean(KL)\n",
    "    reconst_err_mean = tf.reduce_mean(reconst_err)\n",
    "\n",
    "    # Dual loss\n",
    "    d_loss_d = tf.reduce_mean(\n",
    "      tf.nn.sigmoid_cross_entropy_with_logits(logits=Td, labels=tf.ones_like(Td)))\n",
    "    d_loss_i = tf.reduce_mean(\n",
    "      tf.nn.sigmoid_cross_entropy_with_logits(logits=Ti, labels=tf.zeros_like(Ti)))\n",
    "    loss_dual = d_loss_i + d_loss_d\n",
    "    # 获取需要更新的参数\n",
    "    qvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"encoder\")\n",
    "    pvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"decoder\")\n",
    "    dvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
    "\n",
    "    td = tf.reduce_mean(Td)\n",
    "#     gz = tf.reduce_mean(logz)\n",
    "#     gr = tf.reduce_mean(logr)\n",
    "\n",
    "    return loss_primal, loss_dual, pvars+qvars, dvars, ELBO_mean, reconst_err_mean, KL_mean,td\n",
    "\n",
    "def GetTrainOp(loss_primal, loss_dual, vars_primal, vars_dual):\n",
    "    learning_rate = LR_PRIMAL\n",
    "    learning_rate_adversary = LR_DUAL\n",
    "\n",
    "    # Train step\n",
    "    primal_optimizer = tf.train.AdamOptimizer(learning_rate, use_locking=True, beta1=0.5)\n",
    "    adversary_optimizer = tf.train.AdamOptimizer(learning_rate_adversary, use_locking=True, beta1=0.5)\n",
    "\n",
    "    primal_grads = primal_optimizer.compute_gradients(loss_primal, var_list=vars_primal)\n",
    "    adversary_grads = adversary_optimizer.compute_gradients(loss_dual, var_list=vars_dual)\n",
    "\n",
    "    primal_grads = [(grad, var) for grad, var in primal_grads if grad is not None]\n",
    "    adversary_grads = [(grad, var) for grad, var in adversary_grads if grad is not None]\n",
    "\n",
    "    allgrads = [grad for grad, var in primal_grads + adversary_grads]\n",
    "    with tf.control_dependencies(allgrads):\n",
    "        primal_train_step = primal_optimizer.apply_gradients(primal_grads)\n",
    "        adversary_train_step = adversary_optimizer.apply_gradients(adversary_grads)\n",
    "\n",
    "    train_op = tf.group(primal_train_step, adversary_train_step)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e8bba",
   "metadata": {},
   "source": [
    "### 2.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c001128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global batch_index, h_dim\n",
    "batch_index = 0\n",
    "\n",
    "# 定义一些构建计算图所需函数\n",
    "def Get2_WD(a_e, a_v, b_e, b_v):\n",
    "    '''\n",
    "    传入批量的两个多元高斯分布的均值和方差,(batch_size, z_dim)\n",
    "    计算这一批多元高斯分布的平均距离,根据是否匹配，分别计算距离\n",
    "    和对应的距离向量\n",
    "    '''\n",
    "    ab_e = tf.subtract(a_e, b_e)\n",
    "    ab_v = tf.subtract(a_v, b_v)\n",
    "\n",
    "    ab_e2 = tf.multiply(ab_e, ab_e)\n",
    "    ab_v2 = tf.multiply(ab_v, ab_v)\n",
    "\n",
    "    e_v = tf.add(ab_e2, ab_v2)\n",
    "    e_v_sum = tf.reduce_mean(e_v, 1)\n",
    "\n",
    "    return e_v_sum, e_v\n",
    "\n",
    "# 构建二分类网络结构\n",
    "def ClassifyModel(x):\n",
    "    net = x\n",
    "    h_dim = 300\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.relu)\n",
    "    net = slim.dropout(net, 0.3, is_training=False)\n",
    "\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.relu)\n",
    "    net = slim.dropout(net, 0.3, is_training=False)\n",
    "\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.relu)\n",
    "    net = slim.dropout(net, 0.3, is_training=False)\n",
    "\n",
    "    net = slim.fully_connected(net, h_dim, activation_fn=tf.nn.relu)\n",
    "\n",
    "    prediction = slim.fully_connected(net, 2, activation_fn=tf.nn.softmax)\n",
    "    return prediction\n",
    "\n",
    "classify_model = tf.make_template('ClassifyModel', ClassifyModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9b3ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 保存X矩阵的内容\n",
    "# X = sentences_vector\n",
    "# with open('X_array1_10.txt', 'w') as fp:\n",
    "#   for item in X:\n",
    "#     temp = [str(i) for i in item]\n",
    "#     s = ' '.join(temp)\n",
    "#     fp.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f395d280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66879, 768)\n"
     ]
    }
   ],
   "source": [
    "# 读取保存的X矩阵\n",
    "X = []\n",
    "with open(file_path + 'X_array0_100.txt', 'r') as fp:\n",
    "    temp = fp.readlines()\n",
    "for item in temp:\n",
    "    s = item.strip('\\n')\n",
    "    slist = s.split()\n",
    "    slist = [eval(i) for i in slist]\n",
    "    X.append(slist)\n",
    "X = np.array(X)\n",
    "X = torch.FloatTensor(X).data.numpy()\n",
    "print(X.shape)\n",
    "sentences_vector = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e5ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11996, 768)\n"
     ]
    }
   ],
   "source": [
    "# 提取训练集合测试集当中的数据进行训练SE模型\n",
    "a_id_list = []\n",
    "b_id_list = []\n",
    "for i in range(train_df.shape[0]):\n",
    "    a_id_list.append(train_df.loc[i, 'ltable_id'])\n",
    "    b_id_list.append(train_df.loc[i, 'rtable_id'])\n",
    "for i in range(test_df.shape[0]):\n",
    "    a_id_list.append(test_df.loc[i, 'ltable_id'])\n",
    "    b_id_list.append(test_df.loc[i, 'rtable_id'])\n",
    "a_id_list = list(set(a_id_list))\n",
    "b_id_list = list(set(b_id_list))\n",
    "    \n",
    "X_train_test = []\n",
    "for a_id in a_id_list:\n",
    "    X_train_test.append(sentences_vector[a_id])\n",
    "for b_id in b_id_list:\n",
    "    X_train_test.append(sentences_vector[table_a_df.shape[0] + b_id])\n",
    "\n",
    "X_train_test = np.array(X_train_test)\n",
    "X_train_test = torch.FloatTensor(X_train_test).data.numpy()\n",
    "print(X_train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "308c037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /export/home/lsc/anaconda3/envs/tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-1a56776ab9c7>:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /export/home/lsc/anaconda3/envs/tensorflow1.x/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "## 构建批处理操作\n",
    "BATCH_SIZE = 20\n",
    "Y_fake = torch.linspace(1, 100, X_train_test.shape[0]) # 为X_attr 构建一个虚假的Y值\n",
    "torch_dataset = Data.TensorDataset(torch.FloatTensor(X_train_test), Y_fake)\n",
    "\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "## 定义超参数\n",
    "X_dim = X.shape[1]\n",
    "h_dim = 300\n",
    "z_dim = 200\n",
    "eps_dim = 32\n",
    "eps_nbasis = 16\n",
    "\n",
    "# 获取模型变量\n",
    "encoder = tf.make_template('encoder', encoder_func)\n",
    "decoder = tf.make_template('decoder', decoder_func)\n",
    "discriminator = tf.make_template('discriminator', discriminator_func)\n",
    "\n",
    "# 构建计算图\n",
    "LR_PRIMAL = 1e-5\n",
    "LR_DUAL = 2e-5\n",
    "\n",
    "x_real = tf.placeholder(tf.float32, shape=[BATCH_SIZE, X_dim])\n",
    "loss_primal, loss_dual, vars_primal, vars_dual, ELBO_mean, reconst_err_mean, KL_mean,td = BuildComputationalGraph(x_real)\n",
    "train_op1 = GetTrainOp(loss_primal, loss_dual, vars_primal, vars_dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c2fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_vector = X\n",
    "# 先给train_df添加一个属性列similar:表示两个元组对应向量之间的余弦相似度\n",
    "train_df['similar'] = 0.0\n",
    "\n",
    "# 计算出train_df中所有元组对，对应的余弦相似度\n",
    "for i in range(train_df.shape[0]):\n",
    "    a_id = train_df.loc[i, 'ltable_id']\n",
    "    b_id = train_df.loc[i, 'rtable_id']\n",
    "    em_a = sentences_vector[a_id]\n",
    "    em_b = sentences_vector[table_a_df.shape[0] + b_id]\n",
    "    train_df.loc[i, 'similar'] = float(util.cos_sim(em_a, em_b))\n",
    "\n",
    "for i in range(valid_df.shape[0]):\n",
    "    a_id = valid_df.loc[i, 'ltable_id']\n",
    "    b_id = valid_df.loc[i, 'rtable_id']\n",
    "    em_a = sentences_vector[a_id]\n",
    "    em_b = sentences_vector[table_a_df.shape[0] + b_id]\n",
    "    valid_df.loc[i, 'similar'] = float(util.cos_sim(em_a, em_b))\n",
    "\n",
    "# 分别将匹配的、不匹配的原组对，进行分开\n",
    "pairs_0, pairs_1 = [], []\n",
    "\n",
    "pairs_0.extend(train_df[train_df['label'] == 0].values.tolist())\n",
    "pairs_0.extend(valid_df[valid_df['label'] == 0].values.tolist())\n",
    "\n",
    "pairs_1.extend(train_df[train_df['label'] == 1].values.tolist())\n",
    "pairs_1.extend(valid_df[valid_df['label'] == 1].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7845996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置批处理函数所需的，索引\n",
    "global id_0, id_1, num_0, num_1\n",
    "id_0, id_1 = 0, 0\n",
    "num_0, num_1 = 15, 5\n",
    "total_num01 = num_0 + num_1\n",
    "\n",
    "# 构建新的生产批数据的函数\n",
    "def GetBatchData_2(pairs_0, pairs_1, sentences_vector):\n",
    "    '''\n",
    "    新的批数据生产函数\n",
    "    '''\n",
    "    # 根据batch_size设置pairs_0和pairs_1，每次抽样多少个数据\n",
    "    # pairs_0 每批次15个，pairs_1 每批次5个\n",
    "    global id_0, id_1, num_0, num_1\n",
    "    \n",
    "    len_0, len_1 = len(pairs_0), len(pairs_1)\n",
    "#     move_0 = len_0 // num_0\n",
    "#     move_1 = len_1 // num_1\n",
    "#     move_0, move_1 = 100, 50\n",
    "    x_a, x_b, y = [], [], []\n",
    "    \n",
    "    # 对pairs_0进行采样\n",
    "    for i in range(num_0):\n",
    "        id_0 = (id_0 + 1) % len_0\n",
    "        x_a.append(sentences_vector[ int(pairs_0[id_0][0]) ])\n",
    "        x_b.append(sentences_vector[table_a_df.shape[0] + int(pairs_0[id_0][1]) ])\n",
    "        y.append(int(pairs_0[id_0][2]))\n",
    "        \n",
    "    for i in range(num_1):\n",
    "        id_1 = (id_1 + 1) % len_1\n",
    "        x_a.append(sentences_vector[ int(pairs_1[id_1][0]) ])\n",
    "        x_b.append(sentences_vector[table_a_df.shape[0] + int(pairs_1[id_1][1]) ])\n",
    "        y.append(int(pairs_1[id_1][2]) )\n",
    "    \n",
    "    x_a = np.array(x_a).reshape(total_num01, -1)\n",
    "    x_b = np.array(x_b).reshape(total_num01, -1)\n",
    "    y = np.array(y).reshape(total_num01, -1)\n",
    "    \n",
    "    return x_a, x_b, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c9d278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainClassifyModel(x_a, x_b, y):\n",
    "\n",
    "    # 对数据进行编码处理\n",
    "    ez_a, logstd_a = encoder(x_a)\n",
    "    ez_b, logstd_b = encoder(x_b)\n",
    "    # 这里的vz 表示z的标准差（不修改名字了，因该是std_z_a）\n",
    "    vz_a = tf.exp(logstd_a)\n",
    "    vz_b = tf.exp(logstd_b)\n",
    "\n",
    "    # 计算2-WassersteinDistance, (batch_size, 1), (batch_size, z_dim)\n",
    "    WD_disc, WD_vector = Get2_WD(ez_a, vz_a, ez_b, vz_b)\n",
    "\n",
    "    # 计算 模型的预测,每组数据对应匹配的概率，（batch_size, 2）\n",
    "    # x_data = tf.concat([x_a, x_b], axis=-1)\n",
    "    prediction = classify_model(WD_vector)\n",
    "    predict_0 = tf.slice(prediction, [0,0],[-1,1])\n",
    "    predict_1 = tf.slice(prediction, [0,1],[-1,1])\n",
    "\n",
    "    # 构造对比损失函数-论文当中的,Cost-effictive ...\n",
    "    M = 0.5  # 调节encoder参数更新的超参数\n",
    "    class_w = 1.0  # 增加匹配样例的权重\n",
    "    # 对概率值进行范围限制，防止log()操作出现nan\n",
    "    predict_1 = tf.clip_by_value(predict_1, 1e-8, 1.0)\n",
    "    predict_0 = tf.clip_by_value(predict_0, 1e-8, 1.0)\n",
    "    loss_1 = -tf.add(tf.multiply(3*y, tf.log(predict_1)), tf.multiply(1*(1.0-y), tf.log(predict_0)))\n",
    "    loss_2 = tf.add(tf.multiply(3*y, WD_disc), tf.multiply(1.0-y, tf.maximum(0.0,M-WD_disc)))\n",
    "\n",
    "    loss_1 = tf.reduce_mean(loss_1)\n",
    "    loss_2 = tf.reduce_mean(loss_2)\n",
    "    total_loss = loss_1 + loss_2\n",
    "\n",
    "    # 获取 模型需要更新的参数\n",
    "    encoder_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"encoder\")\n",
    "    classify_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"ClassifyModel\")\n",
    "\n",
    "    # optimizer = tf.train.AdamOptimizer(LR)\n",
    "    # loss1_op = optimizer.minimize(loss_1, var_list=classify_vars+encoder_vars)\n",
    "    # loss2_op = optimizer.minimize(loss_2, var_list=encoder_vars)\n",
    "    train_op2 = GetTrainOp(2*loss_1, 1*loss_2, encoder_vars+classify_vars, encoder_vars)\n",
    "    # train_op = optimizer.minimize(total_loss, var_list=encoder_vars+classify_vars)\n",
    "    return train_op2, loss_1, loss_2\n",
    "\n",
    "LR_PRIMAL = 1e-5\n",
    "LR_DUAL = 2e-5\n",
    "\n",
    "x_a = tf.placeholder(tf.float32, shape=[total_num01, X_dim])\n",
    "x_b = tf.placeholder(tf.float32, shape=[total_num01, X_dim])\n",
    "y = tf.placeholder(tf.float32, shape=[total_num01, 1])\n",
    "train_op2, loss_1, loss_2 = TrainClassifyModel(x_a, x_b, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b63b319c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440ccff7b9194068882ed581f9d7cfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#训练计算图\n",
    "sess = tf.InteractiveSession()  # 创建会话交互类\n",
    "sess.run(tf.global_variables_initializer())  # 初始化变量\n",
    "\n",
    "LR_PRIMAL = 1e-5\n",
    "LR_DUAL = 2e-5\n",
    "\n",
    "batch_index = 0\n",
    "max_batch_n = X_train_test.shape[0] // BATCH_SIZE\n",
    "progress = tqdm_notebook(range(60*max_batch_n))\n",
    "for i in progress:\n",
    "    x = GetBatchData(train_loader)\n",
    "    batch_index = (batch_index + 1) % max_batch_n\n",
    "\n",
    "    # sess.run(train_op1)\n",
    "    _,loss_p,loss_d, r_e,kl,td1 = sess.run([train_op1,loss_primal,loss_dual,reconst_err_mean,KL_mean,td], feed_dict={x_real:x})\n",
    "    progress.set_description('loss_primal= %.4f, loss_dual= %.4f, re=%.4f - ' % (loss_p,loss_d,r_e))\n",
    "\n",
    "# # 保存对应的参数\n",
    "# saver = tf.train.Saver()\n",
    "# saver.save(sess,'AVB_paramter.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ecf02",
   "metadata": {},
   "source": [
    "*   loss_primal= 0.0417, loss_dual= 3.6338, re=1.8010 \n",
    "*   loss_primal= 0.0386, loss_dual= 3.9267, re=1.8149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96727aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取模型测试用的test数据\n",
    "sentences_vector = X\n",
    "X_a = []\n",
    "X_b = []\n",
    "## valid_df，转换为test_df\n",
    "for i in range(test_df.shape[0]):\n",
    "    a_id = test_df.loc[i, 'ltable_id']\n",
    "    b_id = test_df.loc[i, 'rtable_id']\n",
    "    X_a.append(sentences_vector[a_id])\n",
    "    X_b.append(sentences_vector[table_a_df.shape[0] + b_id])\n",
    "X_a = np.array(X_a)\n",
    "X_b = np.array(X_b)\n",
    "\n",
    "Y = test_df['label'].values\n",
    "\n",
    "LR_PRIMAL = 1e-5\n",
    "LR_DUAL = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78080693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3809a0c13a4b4f9e941859045ad75d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1722000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练过程\n",
    "# x_data = np.concatenate([X_a, X_b], axis=-1)\n",
    "id_0, id_1 = 0, 0\n",
    "max_batch_n_2 = train_df.shape[0] // total_num01\n",
    "epoch_n = 2000 * max_batch_n_2\n",
    "progress = tqdm_notebook(range(epoch_n))\n",
    "\n",
    "# temp_i = 0\n",
    "# temp_list = [15,15]\n",
    "for i in progress:\n",
    "    # 更新参数\n",
    "    # sess.run(train_op2)\n",
    "    Xa, Xb, Yy = GetBatchData_2(pairs_0, pairs_1, sentences_vector)\n",
    "    id_0 += 1\n",
    "    id_1 += 1\n",
    "\n",
    "    _, loss1, loss2 = sess.run([train_op2, loss_1, loss_2], feed_dict={x_a:Xa, x_b:Xb, y:Yy})\n",
    "    progress.set_description('loss1 = %.5f, loss2 = %.4f - '%(loss1,loss2))\n",
    "\n",
    "    num_0 = random.randint(8,total_num01)\n",
    "    num_1 = total_num01 - num_0      \n",
    "\n",
    "    # 用测试集进行验证\n",
    "    if (i < max_batch_n_2 * 500):\n",
    "        temp = 3*max_batch_n_2\n",
    "    else:\n",
    "        temp = max_batch_n_2\n",
    "#     temp = max_batch_n_2\n",
    "    if i % temp == 0:\n",
    "        e_a, v_a = encoder(X_a)\n",
    "        e_b, v_b = encoder(X_b)\n",
    "        \n",
    "        v_a = tf.exp(v_a)\n",
    "        v_b = tf.exp(v_b)\n",
    "        \n",
    "        wd_disc, wd_vector = Get2_WD(e_a, v_a, e_b, v_b)\n",
    "\n",
    "        # sess.run(wd_vector)\n",
    "        output = classify_model(wd_vector)\n",
    "        # output = tf.argmax(output, 1)\n",
    "        output = sess.run(output)\n",
    "        out = np.argmax(output, 1)\n",
    "        # 计算交叉熵误差\n",
    "        # accuracy = sum(out == Y) / X_a.shape[0]\n",
    "        # print('epoch = %d, accuracy = %.4f' % (i, accuracy))\n",
    "        # 计算模型效果\n",
    "        precision = metrics.precision_score(Y, out)\n",
    "        recall = metrics.recall_score(Y, out)\n",
    "        f1_score = metrics.f1_score(Y, out)\n",
    "        if f1_score > 0.95:\n",
    "            print('epoch = %d, precision = %.3f, recall = %.3f, f1_score = %.4f'%(i,precision,recall,f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066838d8",
   "metadata": {},
   "source": [
    "*   precision = 0.982, recall = 0.986, f1_score = 0.9843(单属性50次训练)（ACM-S）\n",
    "*   precision = 0.867, recall = 0.929, f1_score = 0.8966(单属性训练100次）（beer-S)\n",
    "*   precision = 1.000, recall = 1.000, f1_score = 1.0000（单属性100次）（Fodor-S)\n",
    "*   precision = 0.982, recall = 0.982, f1_score = 0.9820(单属性100次）（DA-D）\n",
    "*   precision = 0.923, recall = 0.889, f1_score = 0.9057(单属性100次）（iTA-S）（需要提高效果）\n",
    "*   precision = 0.595, recall = 0.757, f1_score = 0.6667()(AB-T)\n",
    "*   precision = 0.922, recall = 0.912, f1_score = 0.9173()(DS-S)(需要重新实验)\n",
    "*   precision = 0.694, recall = 0.926, f1_score = 0.7937()(iTA-D)\n",
    "*   precision = 0.697, recall = 0.852, f1_score = 0.7667()(AG-S)\n",
    "*   precision = 0.930, recall = 0.900, f1_score = 0.9150()(DS-D)(有待提高)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c4a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
